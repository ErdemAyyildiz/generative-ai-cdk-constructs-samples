{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58d35eb-0478-45c0-bbb6-966b404b2d90",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install \"sagemaker==2.207.1\" \"huggingface_hub==0.20.3\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a079b84a-5a52-4bbb-a7d9-4070f2767da0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a570eb5-d1a5-4ab9-aed4-39c4025a1dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!mkdir code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19955c06-29f0-4c07-a264-c678e12e8d6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile code/requirements.txt\n",
    "bitsandbytes==0.42.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a98174-41e6-4332-a4e8-e2127ec6573c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile code/inference.py\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16\n",
    "    )\n",
    "    \n",
    "    processor = AutoProcessor.from_pretrained(model_dir)\n",
    "    model = LlavaForConditionalGeneration.from_pretrained(model_dir, quantization_config=quantization_config, device_map=\"auto\")\n",
    "\n",
    "    return processor, model\n",
    "\n",
    "\n",
    "def predict_fn(data, processor_and_model):\n",
    "    \n",
    "    processor, model = processor_and_model\n",
    "    \n",
    "    # get prompt & parameters\n",
    "    prompt = data.pop(\"question\", data)\n",
    "    image_path = data.pop(\"image\", data)\n",
    "    max_new_tokens = data.pop(\"max_new_tokens\", 200)\n",
    "    \n",
    "    image = Image.open(requests.get(image_path, stream=True).raw)\n",
    "    \n",
    "    inputs = processor(prompt, images=[image], padding=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    output = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    generated_text = processor.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "    # create response\n",
    "    return {\"output\": generated_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d6c3db-65b0-4961-86ac-80340150486e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download\n",
    "import random\n",
    "\n",
    "HF_MODEL_ID=\"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "# download snapshot\n",
    "snapshot_dir = snapshot_download(repo_id=HF_MODEL_ID)\n",
    "\n",
    "# create model dir\n",
    "model_tar = Path(f\"model-{random.getrandbits(16)}\")\n",
    "model_tar.mkdir(exist_ok=True)\n",
    "\n",
    "# copy snapshot to model dir\n",
    "copy_tree(snapshot_dir, str(model_tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe889ab-c81e-4c56-949d-34d3e7dc91ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distutils.dir_util import copy_tree\n",
    "# copy code/ to model dir\n",
    "copy_tree(\"code/\", str(model_tar.joinpath(\"code\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbeacf1-aa47-4aca-8729-055bc4c69e3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "# helper to create the model.tar.gz\n",
    "def compress(tar_dir=None,output_file=\"model.tar.gz\"):\n",
    "    parent_dir=os.getcwd()\n",
    "    os.chdir(tar_dir)\n",
    "    with tarfile.open(os.path.join(parent_dir, output_file), \"w:gz\") as tar:\n",
    "        for item in os.listdir('.'):\n",
    "          print(item)\n",
    "          tar.add(item, arcname=item)\n",
    "    os.chdir(parent_dir)\n",
    "\n",
    "compress(str(model_tar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e0b69d-f0bb-4785-9c08-6cd7f5d52150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "# upload model.tar.gz to s3\n",
    "s3_model_uri=S3Uploader.upload(local_path=\"model.tar.gz\", desired_s3_uri=f\"s3://{sess.default_bucket()}/llava-hf-15-7b-test1\")\n",
    "\n",
    "print(f\"model uploaded to: {s3_model_uri}\")\n",
    "# Take note of the s3_model_uri value, this is what the construct will use to deploy the model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
